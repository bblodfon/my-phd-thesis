# Text {#text}

Here I have various text the I write at times and I will include most probably in my completed thesis report:

## Precision (PPV) as a metric for pharmaceutical companies

I think if you say the following: I have 1000 drugs in my lab/company, that is almost 500.000 pairwise combinations. 
I want to screen combinations for synergies, but I only have screening capacity for 100.000 combinations, so there are 400.000 combinations I can't screen. 

**How do I choose the 100.000 combinations to screen?**

Let's say there are in fact 50.000 synergies, meaning a prevalence of 10%.
There are 3 alternatives (using the pipeline):

1. Blindly.
You will get a prevalence of synergies pretty close to the prevalence of synergies among the 500.000, so you will get 10.000 synergistic pairs.
2. Screen by a **topology guided prediction (random models)**.
You will get maybe 13% if I remember correctly for atopo, i.e. 13.000, meaning 3000 more than blinded
3. Screen by a **simulation guided prediction (fit to steady state/calibrated models)**.
You will get maybe 20%, i.e. 20.000, which is 10.000 (double) than blinded, and 7.000 more than guided by topology.

## Why automated topology?

My take on this:

1. Yes, one of the reasons is advantage in the simulations in the sense that when you have logical models that have no inputs you are statistically more able to have models with a few/less (even better: 1) stable state(s). 
See it like this: if a logical model has one input (let's say node X) then it's sure that this logical model will have one attractor with X:0 and one with X:1, be it a stable state or a more complex attractor. 
Two inputs, 4 attractors, etc. (Denis helped me realise this actually on a talk in Athen's ECCB, great times)
2. The second reason that Asmund told me about when I asked him the same thing, was one of the most basic hypothesis behind his modeling - which sums up to *where cancer comes from*. 
By using a no-inputs topology we adhere to the principle that cancer is something that relates to the system itself and not to the external interactions of the system. 
It comes from dysregulations within related to "broken" circuits, etc. 
It is in contrast with the traditional view on the same thing that experimentalists used to understand cancer: I perturb the system (cell) by inhibiting a specific hormone/receptor (input) and see how it reacts (and where most modeling approaches are based on).

Asmund's take on this:
1. few stable states
2. cancer is a system disease in itself (not related to e.g. external hormones, they are present also for healthy cells, it is something in the cancer cell that allows it to sense the external hormone differently than healthy cells).

In addition (and maybe related to point 1):
3. In 'traditional' modeling a system is defined to respond in a certain way to a set of specified 'inputs' (I mean not here model input nodes but rather a configuration of the model, e.g. ERK is active). 
A self-contained topology merges the input condition and the output response in a single observable entity: When the system is initialized in its stable state it will remain there. 
Therefore observation of baseline signaling is both the input and the output, reduces need for perturbations.

4. In addition to few stable states (point 1) I believe a self contained topology also means few possible parmeterizations. 
I don't have any mathematical proof of this but it seems reasonable to me.

## VSM-related

Steven's email about VSM and reasoning:

VSM is about a drive to marry the model-driven & the observation-driven worlds of mathematics, into real-world applications.
This is becoming apparent in my own work on VSM: it is a pursuit to more closely emulate human thinking - as our brains clearly manage to integrate both worlds. 
To mimic this integration we need better tools. 
VSM started off as the design for a high-level, intuitive, universal Knowledge Representation, that makes it easier to manage the model-driven world when working with heterogenous, context-rich knowledge. 
(It boils down a thought (not language) onto basic conceptual structures). 
And now it appears to naturally follow from the semantics of VSM that it requires observation-based, defeasable (not-only-logical) reasoning. 
I.e. it invites for a mechanism whereby "hard"-ish prior semantic knowledge (rules) gets extended with "soft" tentative assumptions, prioritized based on collected episodic knowledge (formalized observations). 
And perhaps, these observations may even be synthesized into new guiding rules as well.

I think that biological systems are extreme in their magnitude of complexity, where hundreds of thousands of diverse causalities can all work together. 
To understand them, we need not only big-data machine learning to interpret large-scale data (from new, more local observations), but also defeasible symbolic-like reasoning, as a guide through our diverse and large-scale prior knowledge.

## Acknowledgements {-}

To Asmund for good supervision: 

- Best writing tip: Every paragraph should start with a 'why', next 'what done', next 'what found', next 'what does this mean' => every paragraph is like a small paper, intro + methods + results + discussion

To Martin, firstly for taking me in as a PhD student and giving me the opportunity to come to Norway. Secondly for his strategical advice during my PhD,  
To Astrid Laergid, for letting me know about the RRI course!
And subsequently to Roger Stran, Heindrun Am and all the other participants of the RRI course: mpla mpla for the expericence and making me realise the complexity of science and the beauty of its philoshophy.
To Steven for teaching how to be a write good open-source Web developer and for some nice conversations about context, knowledge representation
To Noemi Del Toro Ayllon,
To IntAct and Henning for letting me be part 



